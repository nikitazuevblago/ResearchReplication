{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Download image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# URL of a random image\n",
    "url = 'https://lmb.informatik.uni-freiburg.de/people/dosovits/Dosovitskiy_photo.JPG'\n",
    "\n",
    "# Download the image\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Resize and convert to tensor\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "img_tensor = img_transform(img)\n",
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain a 1D sequence from a 2D image for input to the ViT encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Split image tensor into patches using CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 14, 14])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_size = 16\n",
    "\n",
    "# Hidden size D (dimensions)\n",
    "vector_size = 768\n",
    "\n",
    "patcher = nn.Conv2d(in_channels=3, out_channels=vector_size,\n",
    "                    kernel_size=patch_size, stride=patch_size)\n",
    "patched_img = patcher(img_tensor)\n",
    "patched_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Flatten 2d patches and permute dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196, 768])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatter = nn.Flatten(start_dim=1)\n",
    "\n",
    "# Changing dimensions due to input requirements of MSA (multi-head self attention)\n",
    "flattened_patches = flatter(patched_img).permute(1,0)\n",
    "flattened_patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepend extra learnable [class] embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([197, 768])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_token = nn.Parameter(torch.randn(1, 768))\n",
    "flattened_patches = torch.cat((class_token, flattened_patches))\n",
    "flattened_patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add position embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 dimensions of first patch of flattened_patches, pos_embs and their sum\n",
      "\n",
      " tensor([1.1299, 0.9617, 0.6921], grad_fn=<SliceBackward0>)\n",
      " tensor([ 1.6853, -0.3063,  0.5664], grad_fn=<SliceBackward0>)\n",
      " tensor([3.1281, 0.7282, 1.3983], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pos_embs = nn.Parameter(torch.randn(flattened_patches.shape))\n",
    "encoder_input =  pos_embs + flattened_patches\n",
    "\n",
    "dropout_rate = 0.1\n",
    "embedding_dropout = nn.Dropout(dropout_rate)\n",
    "encoder_input = embedding_dropout(encoder_input)\n",
    "\n",
    "print(f'First 3 dimensions of first patch of flattened_patches, pos_embs and their sum\\\n",
    "\\n\\n {flattened_patches[0][:3]}\\n {pos_embs[0][:3]}\\n {encoder_input[0][:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ViT encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Layer Normalization, Multi-head Self-Attention and Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized embeddings:\n",
      "    tensor([2.0638, 0.4959, 0.9337], grad_fn=<SliceBackward0>)\n",
      "Contextualized embeddings:\n",
      "    tensor([ 0.0149, -0.0182,  0.0260], grad_fn=<SliceBackward0>)\n",
      "Embeddings after first skip or residual connection:\n",
      "   tensor([3.1429, 0.7100, 1.4243], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "LN1 = nn.LayerNorm(vector_size)\n",
    "normalized_vecs = LN1(encoder_input)\n",
    "print(f'Normalized embeddings:\\n    {normalized_vecs[0][:3]}')\n",
    "\n",
    "MSA = nn.MultiheadAttention(batch_first=True, num_heads=12, embed_dim=vector_size)\n",
    "contextualized_embs, _ = MSA(query=normalized_vecs, key=normalized_vecs, \n",
    "                            value=normalized_vecs, need_weights=False)\n",
    "print(f'Contextualized embeddings:\\n    {contextualized_embs[0][:3]}')\n",
    "\n",
    "intermediate_embeddings = encoder_input + contextualized_embs\n",
    "print(f'Embeddings after first skip or residual connection:\\n   {intermediate_embeddings[0][:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Layer Normalization, Multi Layer Perceptron and Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intermediate_embeddings = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
