{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alenazueva/PycharmProjects/ResearchReplication/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')  # Add the parent directory to sys.path\n",
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, LambdaLR, SequentialLR\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from train import create_dataloaders, train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Download image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# URL of a random image\n",
    "url = 'https://lmb.informatik.uni-freiburg.de/people/dosovits/Dosovitskiy_photo.JPG'\n",
    "\n",
    "# Download the image\n",
    "response = requests.get(url)\n",
    "img = Image.open(BytesIO(response.content))\n",
    "\n",
    "# Resize and convert to tensor\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224,224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "img_tensor = img_transform(img)\n",
    "img_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain a 1D sequence from a 2D image for input to the ViT encoder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Split image tensor into patches using CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 14, 14])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patch_size = 16\n",
    "\n",
    "# Hidden size D (dimensions)\n",
    "vector_size = 768\n",
    "\n",
    "patcher = nn.Conv2d(in_channels=3, out_channels=vector_size,\n",
    "                    kernel_size=patch_size, stride=patch_size)\n",
    "patched_img = patcher(img_tensor)\n",
    "patched_img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Flatten 2d patches and permute dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([196, 768])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flatter = nn.Flatten(start_dim=1)\n",
    "\n",
    "# Changing dimensions due to input requirements of MSA (multi-head self attention)\n",
    "flattened_patches = flatter(patched_img).permute(1,0)\n",
    "flattened_patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepend extra learnable [class] embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([197, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_token = nn.Parameter(torch.randn(1, 768))\n",
    "flattened_patches = torch.cat((class_token, flattened_patches))\n",
    "flattened_patches.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Add position embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 3 dimensions of first patch of flattened_patches, pos_embs and their sum\n",
      "\n",
      " tensor([-0.2942, -0.1906, -0.5862], grad_fn=<SliceBackward0>)\n",
      " tensor([-0.6299,  0.4337,  0.7066], grad_fn=<SliceBackward0>)\n",
      " tensor([-1.0267,  0.2700,  0.1338], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pos_embs = nn.Parameter(torch.randn(flattened_patches.shape))\n",
    "encoder_input =  pos_embs + flattened_patches\n",
    "\n",
    "dropout_rate = 0.1\n",
    "embedding_dropout = nn.Dropout(dropout_rate)\n",
    "encoder_input = embedding_dropout(encoder_input)\n",
    "\n",
    "print(f'First 3 dimensions of first patch of flattened_patches, pos_embs and their sum\\\n",
    "\\n\\n {flattened_patches[0][:3]}\\n {pos_embs[0][:3]}\\n {encoder_input[0][:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ViT encoder "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Layer Normalization, Multi-head Self-Attention and Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized embeddings:\n",
      "    tensor([[-0.7083,  0.1522,  0.0618]], grad_fn=<SliceBackward0>)\n",
      "Contextualized embeddings:\n",
      "    tensor([[ 0.0124, -0.0056,  0.1641]], grad_fn=<SliceBackward0>)\n",
      "Embeddings after first skip or residual connection:\n",
      "   tensor([[-1.0144,  0.2644,  0.2979]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "LN1 = nn.LayerNorm(vector_size)\n",
    "normalized_vecs = LN1(encoder_input).unsqueeze(0)\n",
    "print(f'Normalized embeddings:\\n    {normalized_vecs[:,0,:3]}')\n",
    "\n",
    "MSA = nn.MultiheadAttention(batch_first=True, num_heads=12, embed_dim=vector_size)\n",
    "contextualized_embs, _ = MSA(query=normalized_vecs, key=normalized_vecs, \n",
    "                            value=normalized_vecs, need_weights=False)\n",
    "print(f'Contextualized embeddings:\\n    {contextualized_embs[:,0,:3]}')\n",
    "\n",
    "intermediate_embeddings = encoder_input + contextualized_embs\n",
    "print(f'Embeddings after first skip or residual connection:\\n   {intermediate_embeddings[:,0,:3]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Layer Normalization, Multi Layer Perceptron and Residual connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 197, 768])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LN2 = nn.LayerNorm(vector_size)\n",
    "normalized_vecs = LN2(intermediate_embeddings)\n",
    "\n",
    "class MLP_block(nn.Module):\n",
    "    def __init__(self, MLP_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features=vector_size, out_features=MLP_size)\n",
    "        self.gelu = nn.GELU('tanh')\n",
    "        self.fc2 = nn.Linear(in_features=MLP_size, out_features=vector_size)\n",
    "\n",
    "        # Applied after each dense layer\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.fc1(x))\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "MLP = MLP_block(MLP_size=3072, dropout_rate=dropout_rate)\n",
    "mlp_embeddings = MLP(normalized_vecs)\n",
    "encoder_out_embeddings = intermediate_embeddings + mlp_embeddings\n",
    "encoder_out_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create classification head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Adjustable classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4039,  0.1328]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = ['sausage', 'not a sausage']\n",
    "\n",
    "LNf = nn.LayerNorm(vector_size)\n",
    "normalized_embs = LNf(encoder_out_embeddings)\n",
    "\n",
    "# Single linear layer used during fine-tuning (while pre-training was used MLP with one hidden layer)\n",
    "classifier = nn.Linear(in_features=vector_size, out_features=len(classes))\n",
    "\n",
    "# Put [class] token in classifier head\n",
    "logits = classifier(normalized_embs[:,0])\n",
    "logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create ViT class (for fine-tuning) using steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, MLP_size, num_heads, vector_size, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.LN1 = nn.LayerNorm(vector_size)\n",
    "        self.MSA = nn.MultiheadAttention(batch_first=True, num_heads=num_heads, embed_dim=vector_size)\n",
    "        self.LN2 = nn.LayerNorm(vector_size)\n",
    "        self.mlp = MLP_block(MLP_size, dropout_rate=dropout_rate)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        normalized_vecs = self.LN1(x)\n",
    "        contextualized_embs, _ = self.MSA(query=normalized_vecs, key=normalized_vecs,\n",
    "                                            value=normalized_vecs, need_weights=False)\n",
    "        intermediate_embeddings = x + contextualized_embs\n",
    "        normalized_vecs = self.LN2(intermediate_embeddings)\n",
    "        x = self.mlp(normalized_vecs)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViTbasehybrid(nn.Module):\n",
    "    def __init__(self, num_labels:int, img_size:tuple[int, int], patch_size:int=16, \n",
    "                    dropout_rate:float=0.1, vector_size:int=768, num_heads:int=12, \n",
    "                    num_transformer_layers:int=12, MLP_size:int=3072):\n",
    "        super().__init__()\n",
    "\n",
    "        # Obtain 1d seq from 2d images\n",
    "        self.patcher = nn.Conv2d(in_channels=3, out_channels=vector_size,\n",
    "                    kernel_size=patch_size, stride=patch_size)\n",
    "        self.flatter = nn.Flatten(start_dim=2) #start_dim=2 because we added batch_size dim\n",
    "        self.class_token = nn.Parameter(torch.randn(1, 1, vector_size), requires_grad=True)\n",
    "        assert (img_size[0]*img_size[1]) % patch_size == 0, 'Img size must be divisible by patch_size!'\n",
    "        number_of_patches = int((img_size[0]*img_size[1]) / patch_size**2)\n",
    "        self.pos_embs = nn.Parameter(torch.randn(1, number_of_patches+1, vector_size), requires_grad=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Create encoder blocks\n",
    "        self.transformer_encoder = nn.Sequential(*[ViTEncoder(MLP_size,num_heads,\n",
    "                                                              vector_size,dropout_rate) \n",
    "                                                    for layer in range(num_transformer_layers)])\n",
    "\n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(nn.LayerNorm(vector_size),\n",
    "                         nn.Linear(in_features=vector_size, out_features=num_labels))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = self.patcher(x)\n",
    "        x = self.flatter(x).permute(0,2,1)\n",
    "\n",
    "        # Making class token suitable for current batch_size\n",
    "        cls_token = self.class_token.expand(batch_size, -1, -1)\n",
    "        \n",
    "        x = torch.cat((cls_token, x), dim=1)\n",
    "        x = self.pos_embs + x\n",
    "        x = self.dropout(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.classifier(x[:, 0])\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "========================================================================================================================\n",
       "Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable\n",
       "========================================================================================================================\n",
       "ViTbasehybrid (ViTbasehybrid)            [32, 3, 224, 224]    [32, 1000]           152,064              True\n",
       "├─Conv2d (patcher)                       [32, 3, 224, 224]    [32, 768, 14, 14]    590,592              True\n",
       "├─Flatten (flatter)                      [32, 768, 14, 14]    [32, 768, 196]       --                   --\n",
       "├─Dropout (dropout)                      [32, 197, 768]       [32, 197, 768]       --                   --\n",
       "├─Sequential (transformer_encoder)       [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    └─ViTEncoder (0)                    [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─LayerNorm (LN1)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MultiheadAttention (MSA)     --                   [32, 197, 768]       2,362,368            True\n",
       "│    │    └─LayerNorm (LN2)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MLP_block (mlp)              [32, 197, 768]       [32, 197, 768]       4,722,432            True\n",
       "│    └─ViTEncoder (1)                    [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─LayerNorm (LN1)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MultiheadAttention (MSA)     --                   [32, 197, 768]       2,362,368            True\n",
       "│    │    └─LayerNorm (LN2)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MLP_block (mlp)              [32, 197, 768]       [32, 197, 768]       4,722,432            True\n",
       "│    └─ViTEncoder (2)                    [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─LayerNorm (LN1)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MultiheadAttention (MSA)     --                   [32, 197, 768]       2,362,368            True\n",
       "│    │    └─LayerNorm (LN2)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MLP_block (mlp)              [32, 197, 768]       [32, 197, 768]       4,722,432            True\n",
       "│    └─ViTEncoder (3)                    [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─LayerNorm (LN1)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MultiheadAttention (MSA)     --                   [32, 197, 768]       2,362,368            True\n",
       "│    │    └─LayerNorm (LN2)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MLP_block (mlp)              [32, 197, 768]       [32, 197, 768]       4,722,432            True\n",
       "│    └─ViTEncoder (4)                    [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─LayerNorm (LN1)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MultiheadAttention (MSA)     --                   [32, 197, 768]       2,362,368            True\n",
       "│    │    └─LayerNorm (LN2)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MLP_block (mlp)              [32, 197, 768]       [32, 197, 768]       4,722,432            True\n",
       "│    └─ViTEncoder (5)                    [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─LayerNorm (LN1)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MultiheadAttention (MSA)     --                   [32, 197, 768]       2,362,368            True\n",
       "│    │    └─LayerNorm (LN2)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MLP_block (mlp)              [32, 197, 768]       [32, 197, 768]       4,722,432            True\n",
       "│    └─ViTEncoder (6)                    [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─LayerNorm (LN1)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MultiheadAttention (MSA)     --                   [32, 197, 768]       2,362,368            True\n",
       "│    │    └─LayerNorm (LN2)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MLP_block (mlp)              [32, 197, 768]       [32, 197, 768]       4,722,432            True\n",
       "│    └─ViTEncoder (7)                    [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─LayerNorm (LN1)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MultiheadAttention (MSA)     --                   [32, 197, 768]       2,362,368            True\n",
       "│    │    └─LayerNorm (LN2)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MLP_block (mlp)              [32, 197, 768]       [32, 197, 768]       4,722,432            True\n",
       "│    └─ViTEncoder (8)                    [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─LayerNorm (LN1)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MultiheadAttention (MSA)     --                   [32, 197, 768]       2,362,368            True\n",
       "│    │    └─LayerNorm (LN2)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MLP_block (mlp)              [32, 197, 768]       [32, 197, 768]       4,722,432            True\n",
       "│    └─ViTEncoder (9)                    [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─LayerNorm (LN1)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MultiheadAttention (MSA)     --                   [32, 197, 768]       2,362,368            True\n",
       "│    │    └─LayerNorm (LN2)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MLP_block (mlp)              [32, 197, 768]       [32, 197, 768]       4,722,432            True\n",
       "│    └─ViTEncoder (10)                   [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─LayerNorm (LN1)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MultiheadAttention (MSA)     --                   [32, 197, 768]       2,362,368            True\n",
       "│    │    └─LayerNorm (LN2)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MLP_block (mlp)              [32, 197, 768]       [32, 197, 768]       4,722,432            True\n",
       "│    └─ViTEncoder (11)                   [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─LayerNorm (LN1)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MultiheadAttention (MSA)     --                   [32, 197, 768]       2,362,368            True\n",
       "│    │    └─LayerNorm (LN2)              [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "│    │    └─MLP_block (mlp)              [32, 197, 768]       [32, 197, 768]       4,722,432            True\n",
       "├─Sequential (classifier)                [32, 768]            [32, 1000]           --                   True\n",
       "│    └─LayerNorm (0)                     [32, 768]            [32, 768]            1,536                True\n",
       "│    └─Linear (1)                        [32, 768]            [32, 1000]           769,000              True\n",
       "========================================================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 5.54\n",
       "========================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3292.46\n",
       "Params size (MB): 232.27\n",
       "Estimated Total Size (MB): 3543.99\n",
       "========================================================================================================================"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "# Setting 1000 images as in ImageNet\n",
    "custom_vit = ViTbasehybrid(num_labels=1000, img_size=tuple(img_tensor.shape[1:]))\n",
    "\n",
    "summary(model=custom_vit,\n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As it can be seen in output the \"Total Parameters\" amount equal to 86,567,656. \n",
    "## Same as in torchvision.models.vit_b_16()\n",
    "## The model is successfuly replicated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "============================================================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
       "============================================================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [32, 3, 224, 224]    [32, 1000]           768                  True\n",
       "├─Conv2d (conv_proj)                                         [32, 3, 224, 224]    [32, 768, 14, 14]    590,592              True\n",
       "├─Encoder (encoder)                                          [32, 197, 768]       [32, 197, 768]       151,296              True\n",
       "│    └─Dropout (dropout)                                     [32, 197, 768]       [32, 197, 768]       --                   --\n",
       "│    └─Sequential (layers)                                   [32, 197, 768]       [32, 197, 768]       --                   True\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [32, 197, 768]       [32, 197, 768]       7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [32, 197, 768]       [32, 197, 768]       7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [32, 197, 768]       [32, 197, 768]       7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [32, 197, 768]       [32, 197, 768]       7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [32, 197, 768]       [32, 197, 768]       7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [32, 197, 768]       [32, 197, 768]       7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [32, 197, 768]       [32, 197, 768]       7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [32, 197, 768]       [32, 197, 768]       7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [32, 197, 768]       [32, 197, 768]       7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [32, 197, 768]       [32, 197, 768]       7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [32, 197, 768]       [32, 197, 768]       7,087,872            True\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [32, 197, 768]       [32, 197, 768]       7,087,872            True\n",
       "│    └─LayerNorm (ln)                                        [32, 197, 768]       [32, 197, 768]       1,536                True\n",
       "├─Sequential (heads)                                         [32, 768]            [32, 1000]           --                   True\n",
       "│    └─Linear (head)                                         [32, 768]            [32, 1000]           769,000              True\n",
       "============================================================================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 5.54\n",
       "============================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3330.99\n",
       "Params size (MB): 232.27\n",
       "Estimated Total Size (MB): 3582.53\n",
       "============================================================================================================================================"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model=torchvision.models.vit_b_16(),\n",
    "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (197) must match the size of tensor b (577) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     42\u001b[0m max_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_vit\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/ResearchReplication/ViT/../train.py:179\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, loss_fn, optimizer, device, batch_size, dropout_rate, epochs, max_norm, scheduler)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,epochs\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# Train step\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m scheduler:\n\u001b[0;32m--> 179\u001b[0m         train_loss, train_acc, train_precision, train_recall, train_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m                                                                                    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m         train_loss, train_acc, train_precision, train_recall, train_f1 \u001b[38;5;241m=\u001b[39m train_step(model,train_dataloader,loss_fn,optimizer,\n\u001b[1;32m    183\u001b[0m                                                                                         device,max_norm)\n",
      "File \u001b[0;32m~/PycharmProjects/ResearchReplication/ViT/../train.py:42\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device, max_norm, epoch, scheduler)\u001b[0m\n\u001b[1;32m     39\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# 1. Forward pass\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# 2. Compute loss\u001b[39;00m\n\u001b[1;32m     45\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(logits, y)\n",
      "File \u001b[0;32m~/PycharmProjects/ResearchReplication/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/ResearchReplication/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[10], line 56\u001b[0m, in \u001b[0;36mViTbasehybrid.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m cls_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_token\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     55\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((cls_token, x), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_embs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n\u001b[1;32m     57\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m     58\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (197) must match the size of tensor b (577) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Prepare dataloaders\n",
    "data_dir = '../' / Path('CV_test_data/')\n",
    "train_threshold = 0.8\n",
    "batch_size = 32\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(size=(384,384)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataloader, test_dataloader = create_dataloaders(data_dir, train_threshold, batch_size, transform)\n",
    "\n",
    "# Fine-tune the model (using parameters for fine-tuning mentioned in Research Paper)\n",
    "## Except for batch_size and used Polyak & Juditsky (1992) averaging\n",
    "num_labels = len([folder for folder in os.listdir(data_dir) \n",
    "                                if (data_dir / folder).is_dir()])\n",
    "test_vit = ViTbasehybrid(num_labels=num_labels, img_size=tuple(img_tensor.shape[1:]))\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "initial_lr = 0.001\n",
    "optimizer = optim.SGD(params=custom_vit.classifier.parameters(), lr=initial_lr,\n",
    "                        weight_decay=0, momentum=0.9)\n",
    "def lr_lambda(step):\n",
    "    if step < 125:\n",
    "        return 0.001 / initial_lr\n",
    "    elif step < 250:\n",
    "        return 0.003 / initial_lr\n",
    "    elif step < 375:\n",
    "        return 0.01 / initial_lr\n",
    "    else:\n",
    "        return 0.03 / initial_lr\n",
    "\n",
    "warmup_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "decay_scheduler = CosineAnnealingLR(optimizer, T_max=100)\n",
    "# Combine schedulers using SequentialLR\n",
    "scheduler = SequentialLR(optimizer, schedulers=[warmup_scheduler, decay_scheduler], milestones=[500])\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "batch_size = 32\n",
    "dropout_rate = 0.1\n",
    "epochs = 3\n",
    "max_norm = 1.0\n",
    "\n",
    "train(test_vit,train_dataloader,test_dataloader,\n",
    "        loss_fn,optimizer,device,batch_size,dropout_rate,epochs,\n",
    "        max_norm=max_norm,scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
